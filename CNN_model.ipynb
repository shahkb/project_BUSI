{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification of Breast Ultra Sound Images\n",
    "\n",
    "Here we experiment with classical algorithms such as Logistic Regression, SVM and Bag of Visual words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from utils import init_img_dict, get_file_dicts, filter_files, find_mask, print_ndarray_info\n",
    "from utils import img_read, img_write, img_resize, img_flip, comp_fft, histogram_equalization\n",
    "from utils import display_img, display_img_list_3, display_3_imgs, display_3_hist, resize_imgs, flip_imgs, append_img_data\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, classification_report, confusion_matrix \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "import talos\n",
    "from talos.utils import lr_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to assess model quality and to plot confusion matrix, ROC etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix (binary)\n",
    "def plot_cm(cm):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Plot ROC\n",
    "def plot_ROC(fpr, tpr, auc):\n",
    "\n",
    "    plt.figure(figsize = (6.4, 6.4))\n",
    "    plt.plot(fpr, tpr, label= f\"ROC, auc= {auc:.2f}\")\n",
    "    plt.title(\"ROC curve\")\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# To assess model quality analysis\n",
    "def model_quality(model, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # Accuracy\n",
    "    print(f\"Model accuracy on training data: {model.score(X_train, y_train):.2f} \")\n",
    "    print(f\"Model accuracy on test data: {model.score(X_test, y_test):.2f} \")\n",
    "\n",
    "    # Classification report (precision, recall, f1 score)\n",
    "    print(f\"\\nClassification report:\\n {classification_report(y_test, model.predict(X_test))} \")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "    print(f\"Confusion matrix:\\n {cm}\\n\")\n",
    "    plot_cm(cm)\n",
    "\n",
    "    # ROC curve\n",
    "    y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    plot_ROC(fpr, tpr, auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read image list (with dict object for each image) for the 2 classes (Benign and Malignant)\n",
    " - Read benign and malignant dataset into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in benign dataset: 437\n",
      "Number of images in malignant dataset: 420\n"
     ]
    }
   ],
   "source": [
    "img_res = 256\n",
    "\n",
    "benign_img_dir = './Dataset_BUSI_with_GT/benign_256'\n",
    "malignant_img_dir = './Dataset_BUSI_with_GT/malignant_256'\n",
    "\n",
    "# Get a list of images in the images directory\n",
    "benign_img_list_all = get_file_dicts(benign_img_dir)\n",
    "num_benign_img = len(benign_img_list_all)\n",
    "print(f\"Number of images in benign dataset: {num_benign_img}\")\n",
    "\n",
    "malignant_img_list_all = get_file_dicts(malignant_img_dir)\n",
    "num_malignant_img = len(malignant_img_list_all)\n",
    "print(f\"Number of images in malignant dataset: {num_malignant_img}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for classification\n",
    " - Assemble feature data (pixels) and target data (0 for benign and 1 for malignant)\n",
    " - Shuffle the data\n",
    " - Split data in to training and test (70-30) set\n",
    " - Scale the data (use Standard scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_images(img_list, label):\n",
    "    images = []\n",
    "    masks = []\n",
    "    labels = []\n",
    "    for item in img_list:\n",
    "        img = cv2.imread(item['file_name_fullpath'], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.imread(item['file_masks'][0], cv2.IMREAD_GRAYSCALE)\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "        labels.append(label)\n",
    "    return images, masks, labels\n",
    "\n",
    "# normal_im, normal_mk, n_l = import_images(normal_img_list, 0)\n",
    "benign_im, benign_mk, b_l = import_images(benign_img_list_all, 1)\n",
    "malignant_im, malignant_mk, m_l = import_images(malignant_img_list_all, 2)\n",
    "\n",
    "X = benign_im + malignant_im\n",
    "L = b_l+m_l\n",
    "\n",
    "x_resize=[]\n",
    "img_size=256\n",
    "\n",
    "for x in X:\n",
    "    new_array = cv2.resize(x,(img_size,img_size))\n",
    "    x_resize.append(new_array)\n",
    "x_resize = np.array(x_resize)\n",
    "x_resize = x_resize.reshape(x_resize.shape[0], x_resize.shape[1], x_resize.shape[2], 1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(x_resize,L, test_size=0.1, random_state= 7, stratify= L)\n",
    "# X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=7,  stratify= y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(opt = 'adam', learning_rate = .001):\n",
    "# Create a model and trin\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256, 256,1)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    if opt == 'adagrad':\n",
    "        opt = Adagrad(lr=learning_rate)\n",
    "    elif opt == 'adam':\n",
    "        opt = Adam(lr=learning_rate)\n",
    "    elif opt == 'SGD':\n",
    "        opt = SGD(lr=learning_rate) \n",
    "    model.compile(optimizer = opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # history = model.fit(x=x_train, y=y_train, validation_data=(x_val, y_val), batch_size=params['batch_size'], epochs=params['epochs'], verbose=1)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [16, 32]\n",
    "opt = ['adam', 'adagrad']\n",
    "# 'dropout_rate': [0.0, 0.10, 0.20], 0.30],\n",
    "learning_rate = [.001, .01]\n",
    "epochs = [10, 15, 20]\n",
    "param_grid = dict(opt= opt, learning_rate = learning_rate, batch_size = batch_size,epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, learning_rate=0.0001, opt='adam', verbose=1)\n",
    "grid = GridSearchCV(estimator = model,cv = 3, param_grid = param_grid, verbose = 1)\n",
    "grid_result = grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cv_results(search_results):\n",
    "    print('Best score = {:.4f} using {}'.format(search_results.best_score_, search_results.best_params_))\n",
    "    means = search_results.cv_results_['mean_test_score']\n",
    "    stds = search_results.cv_results_['std_test_score']\n",
    "    params = search_results.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print('mean test accuracy +/- std = {:.4f} +/- {:.4f} with: {}'.format(mean, stdev, param))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cv_results(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = grid_result.best_estimator_ \n",
    "history = mlp.fit(\n",
    "    X_train,\n",
    "    y_train)\n",
    "# history = model.fit(np.array(X_train),np.array(y_train), epochs=10,validation_data=(X_test, np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction on validation dataset \n",
    "y_pred = mlp.predict(X_test)\n",
    "print('Accuracy on validation data = {:.4f}'.format(accuracy_score(y_testl, y_pred)))\n",
    "\n",
    "# plot accuracy on training and validation data\n",
    "df_history = pd.DataFrame(history.history)\n",
    "sns.lineplot(data=df_history[['accuracy','val_accuracy']], palette=\"tab10\", linewidth=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24163/1925986220.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# y_test = np.asarray(y_test).astype('float32').reshape((-1,1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 988\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    989\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "model = create_model(opt = 'adam', learning_rate = .001)\n",
    "# y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "# y_test = np.asarray(y_test).astype('float32').reshape((-1,1))\n",
    "history = model.fit(X_train,y_train,epochs=12,validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model quality analysis\n",
    "# accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(np.array(X_test),np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "predicted = [np.argmax(item) for item in model.predict(X_test)]\n",
    "\n",
    "conf = confusion_matrix(y_test,predicted)\n",
    "conf\n",
    "\n",
    "info = [\n",
    "    'benign'   ,  # 0\n",
    "    'malignant',  # 1\n",
    "]\n",
    "plt.figure(figsize = (6,6))\n",
    "ax = sns.heatmap(conf, cmap=plt.cm.Blues, annot=True, square=True, xticklabels = info, yticklabels = info)\n",
    "ax.set_ylabel('Actual', fontsize=40)\n",
    "ax.set_xlabel('Predicted', fontsize=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bbb2307e17ea36bb17521abc68671a87486d60f5db5f8ffac2e3b843695dd9f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
